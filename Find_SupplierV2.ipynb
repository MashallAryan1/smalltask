{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the supplier of an OCR generated invoice using knn and augmented data from NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/mashallaryan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/mashallaryan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/mashallaryan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import ast\n",
    "import sklearn\n",
    "from sklearn.neighbors import NearestNeighbors    \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from joblib import dump, load\n",
    "import re\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "import nlpaug.augmenter.char as nac\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "# Knn file name\n",
    "SKLEARN_MODEL_FILENAME = 'model.ml'\n",
    "\n",
    "# Tfidf vectorizer file name\n",
    "VECTORIZER_FILENAME = 'vect.ml'\n",
    "\n",
    "AUG_FILE = 'augfile.csv'\n",
    "SUP_FILE ='supfile.csv'\n",
    "MAXNUM_AUG = 1000\n",
    "\n",
    "LOG_FILE = 'logfile.log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "fh = logging.FileHandler(LOG_FILE)\n",
    "fh.setLevel(logging.DEBUG)\n",
    "\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "\n",
    "logger.addHandler(fh)\n",
    "logger.addHandler(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_invoice(file_name):\n",
    "    \"\"\"\n",
    "    Reads a file of invoice content created by OCR and converts it to a pandas dataframe\n",
    "    \n",
    "    :param file_name: Name of the file containing invoice content\n",
    "    :return: A dataframe of the invoice words \n",
    "    \n",
    "    \"\"\"\n",
    "    with open(file_name,'r') as f:\n",
    "        words = \"[{}]\".format(f.read().strip())\n",
    "    words = words.replace(\"\\n\", \",\")\n",
    "    words = ast.literal_eval(words)\n",
    "    inv_df = pd.DataFrame(words)\n",
    "    # sort the words according to their order in the original doc\n",
    "    inv_df = inv_df.set_index(['page_id','line_id','pos_id'])\n",
    "    inv_df.sort_index(inplace=True)\n",
    "    \n",
    "    return ' '.join(inv_df['word'])\n",
    "\n",
    "\n",
    "lemat = WordNetLemmatizer()\n",
    "def clean(item):\n",
    "    \"\"\"\n",
    "    preprocessing the input string to remove the unwanted characters and substrings     \n",
    "    :param item: input string\n",
    "    :return: a string in which unwanted characters and substrings are removed \n",
    "    \n",
    "    \"\"\"\n",
    "    res = item.encode(\"ascii\", errors=\"ignore\").decode()\n",
    "\n",
    "    res = re.sub('['+string.punctuation+']+','',res).strip()\n",
    "    res = word_tokenize(res.lower())\n",
    "    res = [lemat.lemmatize(item) for item in res if item not in stopwords.words('english')]    \n",
    "    return   \" \".join(res)\n",
    "\n",
    "\n",
    "\n",
    "def augment_data(suppliers):\n",
    "    logger.info('Data Augmentation: this might take some time...')\n",
    "    aug = nac.OcrAug()\n",
    "    \n",
    "    \n",
    "    cols = suppliers.columns.values\n",
    "    aug_df = []    \n",
    "    for ind, row in suppliers.iterrows():    \n",
    "        aug_df += [pd.DataFrame([[row['Id'],new_aug]],columns=cols) \n",
    "                   for new_aug in aug.augment(row['SupplierName'], n=MAXNUM_AUG) ]\n",
    "    aug_df = pd.concat(aug_df+[suppliers]) \n",
    "    return aug_df \n",
    "\n",
    "\n",
    "def train(sup_df, do_augmentation=True ):\n",
    "    \n",
    "    # Create tfidf vectorizer\n",
    "    vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(ngram_range=(1, 4), analyzer='word')\n",
    "    if do_augmentation:\n",
    "        \n",
    "        # Generate extra supplier names by adding som OCR style noise to the original suppliers' names\n",
    "        aug_sup_df = augment_data(sup_df)\n",
    "        \n",
    "        # generate TF-IDF features fot the suppliers\n",
    "        doc_term_matrix = vectorizer.fit_transform(aug_sup_df['SupplierName'])  \n",
    "        \n",
    "        # Save the augmented data somewhere\n",
    "        aug_sup_df[['Id']].to_csv(AUG_FILE)\n",
    "    else:    \n",
    "        doc_term_matrix = vectorizer.fit_transform(sup_df['SupplierName'])  \n",
    "        sup_df[['Id']].to_csv(AUG_FILE)\n",
    "    \n",
    "    # Save suppliers somewhere\n",
    "    sup_df.to_csv(SUP_FILE)\n",
    "    \n",
    "    # Build a KNN model    \n",
    "    nearestnbr = NearestNeighbors(n_neighbors=1).fit(doc_term_matrix ) \n",
    "    \n",
    "    \n",
    "    return nearestnbr, vectorizer\n",
    "\n",
    "def predict(nearestnbr, vectorizer, query):\n",
    "    \n",
    "    # Load the required DataFrames\n",
    "    aug_sup_df = pd.read_csv(AUG_FILE)\n",
    "    sup_df = pd.read_csv(SUP_FILE)\n",
    "    \n",
    "    #Clean the query\n",
    "    query = clean(query)\n",
    "    \n",
    "    #Convert the query to TF-IDF features\n",
    "    query_tfidf = vectorizer.transform(query.split())    \n",
    "    \n",
    "    # Predict the most probable supplier\n",
    "    distances, indices = nearestnbr.kneighbors(query_tfidf)\n",
    "    \n",
    "    \n",
    "    suppliers_index = indices[distances.argmin()]\n",
    "    sup_id = aug_sup_df['Id'].iloc[suppliers_index].values[0]\n",
    "    return sup_id, sup_df[sup_df['Id']==sup_id]['SupplierName'].values[0]\n",
    "\n",
    "\n",
    "\n",
    "# def main(args):\n",
    "#     inv_file = args['invoice']\n",
    "#     sup_file = args['suppliers']\n",
    "    \n",
    "#     # read suppliers names\n",
    "#     sup_df = pd.read_csv(sup_file)\n",
    "#     sup_df['SupplierName_c'] = sup_df['SupplierName'].apply(clean)\n",
    "    \n",
    "    \n",
    "#     nearestnbr, vectorizer = train(sup_df, do_augmentation=True )\n",
    "\n",
    "#     query = read_invoice(inv_file)\n",
    "    \n",
    "#     query = clean(query)\n",
    "    \n",
    "#     return predict(nearestnbr, vectorizer, query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/local/scratch/mashall/scratch/Myprojects/Find_supplier\r\n"
     ]
    }
   ],
   "source": [
    "!pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data Augmentation: this might take some time...\n"
     ]
    }
   ],
   "source": [
    "sup_df = pd.read_csv('suppliernames.txt')\n",
    "\n",
    "sup_df['SupplierName']  = sup_df['SupplierName'].apply(clean)\n",
    "\n",
    "\n",
    "nearestnbr, vectorizer = train(sup_df, do_augmentation=True )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3153303, 'demo company')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = read_invoice('invoice.txt')\n",
    "\n",
    "query = clean(query)\n",
    "\n",
    "\n",
    "predict(nearestnbr, vectorizer, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
