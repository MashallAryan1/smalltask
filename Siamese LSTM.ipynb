{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese Bidirectional LSTM for Finding the Supplier of OCR generated invoice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/mashallaryan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/mashallaryan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/mashallaryan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /home/mashallaryan/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "from joblib import dump, load\n",
    "import re\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "K = tf.keras.backend\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "import nlpaug.augmenter.char as nac\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.regularizers  as reglzr\n",
    "from nltk.corpus import reuters\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "nltk.download('reuters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FILENAME = 'model.ml'\n",
    "\n",
    "AUG_FILE = 'augfile.csv'\n",
    "SUP_FILE ='supfile.csv'\n",
    "MAXNUM_AUG = 1000\n",
    "\n",
    "EMBEDDING_DIM = 10\n",
    "VOCAB_SIZE = 5000\n",
    "MAX_SEQ_LEN = 300\n",
    "BATCH_SIZE = 100\n",
    "LOG_FILE = 'logfile.log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "fh = logging.FileHandler(LOG_FILE)\n",
    "fh.setLevel(logging.DEBUG)\n",
    "\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "\n",
    "logger.addHandler(fh)\n",
    "logger.addHandler(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_invoice(file_name):\n",
    "    \"\"\"\n",
    "    Reads a file of invoice content created by OCR and converts it to a pandas dataframe\n",
    "    \n",
    "    :param file_name: Name of the file containing invoice content\n",
    "    :return: A dataframe of the invoice words \n",
    "    \n",
    "    \"\"\"\n",
    "    with open(file_name,'r') as f:\n",
    "        words = \"[{}]\".format(f.read().strip())\n",
    "    words = words.replace(\"\\n\", \",\")\n",
    "    words = ast.literal_eval(words)\n",
    "    inv_df = pd.DataFrame(words)\n",
    "    # sort the words according to their order in the original doc\n",
    "    inv_df = inv_df.set_index(['page_id','line_id','pos_id'])\n",
    "    inv_df.sort_index(inplace=True)\n",
    "    \n",
    "    return ' '.join(inv_df['word'])\n",
    "\n",
    "\n",
    "lemat = WordNetLemmatizer()\n",
    "def clean(item):\n",
    "    \"\"\"\n",
    "    preprocessing the input string to remove the unwanted characters and substrings     \n",
    "    :param item: input string\n",
    "    :return: a string in which unwanted characters and substrings are removed \n",
    "    \n",
    "    \"\"\"\n",
    "    res = item.encode(\"ascii\", errors=\"ignore\").decode()\n",
    "\n",
    "    res = re.sub('['+string.punctuation+']+','',res).strip()\n",
    "    res = word_tokenize(res.lower())\n",
    "    res = [lemat.lemmatize(item) for item in res if item not in stopwords.words('english')]    \n",
    "    return   \" \".join(res)\n",
    "\n",
    "\n",
    "def build_tokenizer(docs):\n",
    "    logger.info('build tokenizer.')\n",
    "    uwords = set()\n",
    "    docs.str.lower().str.split().apply(uwords.update)\n",
    "    vocab_size = len(uwords)\n",
    "    tknzr = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")    \n",
    "    tknzr.fit_on_texts(uwords)    \n",
    "    return tknzr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_hidden=50, dropout=0.2, recurrent_dropout=0.2):\n",
    "    \n",
    "    # Create the shared encoder\n",
    "    encoder = models.Sequential()\n",
    "    encoder.add(layers.Embedding(VOCAB_SIZE,EMBEDDING_DIM ,input_shape=(MAX_SEQ_LEN,)) )\n",
    "    encoder.add(layers.Bidirectional(layers.LSTM(num_hidden, kernel_regularizer=reglzr.l2(1e-4), dropout=dropout, recurrent_dropout=recurrent_dropout)))\n",
    "    encoder.add(layers.Dense(num_hidden, kernel_regularizer=reglzr.l2(1e-4)))\n",
    "    \n",
    "    \n",
    "    # Invoice input\n",
    "    inv_in = layers.Input(shape=(MAX_SEQ_LEN,),dtype=tf.int32,name='inv_in')\n",
    "    # Supplier input\n",
    "    sup_in = layers.Input(shape=(MAX_SEQ_LEN,),dtype=tf.int32,name='sup_in')\n",
    "    \n",
    "    # Seperated encoders for invoice and supplier\n",
    "    inv_encoder = encoder(inv_in)\n",
    "    sup_encoder = encoder(sup_in)\n",
    "    \n",
    "    # loss layer\n",
    "    L1_layer =  layers.Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "    \n",
    "    L1_distance = L1_layer([inv_encoder, sup_encoder])\n",
    "    \n",
    "    # Add a dense layer with a sigmoid unit to generate the similarity score\n",
    "    prediction = layers.Dense(1,activation='sigmoid')(L1_distance)\n",
    "\n",
    "    \n",
    "    model = models.Model(inputs=[inv_in, sup_in], outputs=[prediction])\n",
    "    model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(2e-3), metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(suppliers, n_samples_per_sup=10):\n",
    "    \"\"\"\n",
    "    Given a dataframe of suppliers, creates a dataset of positive and negative pairs of invoice-supplier, given a dataframe of suppliers.\n",
    "    \n",
    "    :param suppliers: a dataframe (columns= ['Id', 'SupplierName' ]) of supplier names \n",
    "    :param n_samples_per_sup: number of positive and negative pairs generated per supplier name\n",
    "    :return: a Dataframe (columns= ['invoice', 'SupplierName', 'label']) of invoice-supplier pairs \n",
    "             where the label=1 for a positive samples (invoice contains the supplier name) and label=0 \n",
    "             if invoice does not contains the supplier name.\n",
    "    \"\"\"\n",
    "\n",
    "    def inject_pos(item):\n",
    "        point = np.random.randint(0,len(item['invoice'])-1)\n",
    "        \n",
    "        item_text = item['invoice']\n",
    "        item_num = item['num']\n",
    "        return item_text[0:point]+' {} '.format(supplier_names[item_num]) + item_text[point:]\n",
    "    \n",
    "    \n",
    "    txt = nltk.Text(reuters.words())\n",
    "    words =np.array(txt)[np.random.randint(0,1720901, 10000)]\n",
    "    \n",
    "    supplier_names = suppliers['SupplierName'].tolist()    \n",
    "    num_suppliers = len(supplier_names)\n",
    "\n",
    "    df_positive = []\n",
    "    for i, row in suppliers.iterrows():\n",
    "        df_positive += [pd.DataFrame([[i,row['Id'],' '.join(words[np.random.randint(0,10000, np.random.randint(10, MAX_SEQ_LEN))].tolist() )]],columns=['num','Id','invoice']) for i in range(n_samples_per_sup)]\n",
    "    df_positive = pd.concat(df_positive)     \n",
    "    \n",
    "    \n",
    "    df_negative = []\n",
    "    for i, row in suppliers.iterrows():\n",
    "        df_negative += [pd.DataFrame([[i,row['Id'],' '.join(words[np.random.randint(0,10005, np.random.randint(10, MAX_SEQ_LEN))].tolist() )]],columns=['num','Id','invoice']) for i in range(n_samples_per_sup)]\n",
    "        break\n",
    "    df_negative = pd.concat(df_negative)     \n",
    "    \n",
    "\n",
    "    df_positive['invoice'] = df_positive.apply(inject_pos,axis=1)   \n",
    "    \n",
    "\n",
    "    df_positive['label'] = 1\n",
    "    df_negative['label'] = 0\n",
    "    \n",
    "    \n",
    "    df_all = pd.concat([df_positive, df_negative])    \n",
    "    df_all = df_all.join(suppliers.set_index('Id'), on='Id')\n",
    "    \n",
    "    return df_all[['invoice', 'SupplierName', 'label']]\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(suppliers, epochs=50,  num_sample_per_supplier=10):\n",
    "    dataset_df = create_training_data(suppliers, num_sample_per_supplier)\n",
    "    \n",
    "    X = dataset_df[['invoice', 'SupplierName']]\n",
    "    Y = dataset_df['label']\n",
    "    \n",
    "    \n",
    "    tknzr = build_tokenizer(suppliers['SupplierName'])\n",
    "    invoices =tknzr.texts_to_sequences(X['invoice'].str.split().values)\n",
    "    suppliers = tknzr.texts_to_sequences(X['SupplierName'].str.split().values)    \n",
    "    invoices  = pad_sequences(invoices,maxlen=MAX_SEQ_LEN)\n",
    "    suppliers  = pad_sequences(suppliers,maxlen=MAX_SEQ_LEN)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(({'inv_in':tf.cast(invoices,tf.int32),'sup_in':tf.cast(suppliers,tf.int32)} ,tf.cast( Y.values, tf.int32)))\n",
    "      \n",
    "    dataset = dataset.shuffle(len(dataset_df)).batch(BATCH_SIZE)\n",
    "    \n",
    "    model = build_model()\n",
    "    \n",
    "    model.fit(dataset,  epochs=epochs)   \n",
    "    \n",
    "    return model, tknzr \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(model, tknzr,suppliers, query):\n",
    "    \n",
    "\n",
    "    suppliers = tknzr.texts_to_sequences(suppliers['SupplierName'].str.split().values)    \n",
    "    suppliers  =tf.cast( pad_sequences(suppliers,maxlen=MAX_SEQ_LEN), tf.int32)\n",
    "    query = clean(query )\n",
    "    \n",
    "    query = tknzr.texts_to_sequences(query.split())\n",
    "    query = tf.cast(pad_sequences(query,maxlen=MAX_SEQ_LEN),tf.int32)\n",
    "    \n",
    "    result =np.array([model.predict([query], [supplier]) for supplier in suppliers[:]])\n",
    "    return np.argmax(result)\n",
    "\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f64cd5aedc0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for keyword: 1, expecting 2\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f64cd5aedc0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for keyword: 1, expecting 2\n",
      "10/10 [==============================] - 7s 661ms/step - loss: 0.6786 - accuracy: 0.8108\n",
      "Epoch 2/50\n",
      "10/10 [==============================] - 3s 269ms/step - loss: 0.3086 - accuracy: 0.9892\n",
      "Epoch 3/50\n",
      "10/10 [==============================] - 3s 278ms/step - loss: 0.0912 - accuracy: 0.9892\n",
      "Epoch 4/50\n",
      "10/10 [==============================] - 3s 274ms/step - loss: 0.1096 - accuracy: 0.9892\n",
      "Epoch 5/50\n",
      "10/10 [==============================] - 3s 274ms/step - loss: 0.1131 - accuracy: 0.9892\n",
      "Epoch 6/50\n",
      "10/10 [==============================] - 3s 271ms/step - loss: 0.0750 - accuracy: 0.9892\n",
      "Epoch 7/50\n",
      "10/10 [==============================] - 3s 276ms/step - loss: 0.0813 - accuracy: 0.9892\n",
      "Epoch 8/50\n",
      "10/10 [==============================] - 3s 273ms/step - loss: 0.0710 - accuracy: 0.9892\n",
      "Epoch 9/50\n",
      "10/10 [==============================] - 3s 273ms/step - loss: 0.0634 - accuracy: 0.9892\n",
      "Epoch 10/50\n",
      "10/10 [==============================] - 3s 277ms/step - loss: 0.0604 - accuracy: 0.9892\n",
      "Epoch 11/50\n",
      "10/10 [==============================] - 3s 277ms/step - loss: 0.0616 - accuracy: 0.9892\n",
      "Epoch 12/50\n",
      "10/10 [==============================] - 3s 287ms/step - loss: 0.0593 - accuracy: 0.9892\n",
      "Epoch 13/50\n",
      "10/10 [==============================] - 3s 281ms/step - loss: 0.0490 - accuracy: 0.9892\n",
      "Epoch 14/50\n",
      "10/10 [==============================] - 3s 280ms/step - loss: 0.0497 - accuracy: 0.9892\n",
      "Epoch 15/50\n",
      "10/10 [==============================] - 3s 280ms/step - loss: 0.0440 - accuracy: 0.9892\n",
      "Epoch 16/50\n",
      "10/10 [==============================] - 3s 277ms/step - loss: 0.0398 - accuracy: 0.9892\n",
      "Epoch 17/50\n",
      "10/10 [==============================] - 3s 277ms/step - loss: 0.0345 - accuracy: 0.9892\n",
      "Epoch 18/50\n",
      "10/10 [==============================] - 3s 279ms/step - loss: 0.0448 - accuracy: 0.9892\n",
      "Epoch 19/50\n",
      "10/10 [==============================] - 3s 278ms/step - loss: 0.1341 - accuracy: 0.9892\n",
      "Epoch 20/50\n",
      "10/10 [==============================] - 3s 279ms/step - loss: 0.0524 - accuracy: 0.9892\n",
      "Epoch 21/50\n",
      "10/10 [==============================] - 3s 282ms/step - loss: 0.0430 - accuracy: 0.9892\n",
      "Epoch 22/50\n",
      "10/10 [==============================] - 3s 286ms/step - loss: 0.0426 - accuracy: 0.9892\n",
      "Epoch 23/50\n",
      "10/10 [==============================] - 3s 296ms/step - loss: 0.0391 - accuracy: 0.9892\n",
      "Epoch 24/50\n",
      "10/10 [==============================] - 3s 298ms/step - loss: 0.0294 - accuracy: 0.9892\n",
      "Epoch 25/50\n",
      "10/10 [==============================] - 3s 294ms/step - loss: 0.0368 - accuracy: 0.9892\n",
      "Epoch 26/50\n",
      "10/10 [==============================] - 3s 294ms/step - loss: 0.0275 - accuracy: 0.9892\n",
      "Epoch 27/50\n",
      "10/10 [==============================] - 3s 295ms/step - loss: 0.0262 - accuracy: 0.9892\n",
      "Epoch 28/50\n",
      "10/10 [==============================] - 3s 292ms/step - loss: 0.0263 - accuracy: 0.9892\n",
      "Epoch 29/50\n",
      "10/10 [==============================] - 3s 292ms/step - loss: 0.0247 - accuracy: 0.9892\n",
      "Epoch 30/50\n",
      "10/10 [==============================] - 3s 292ms/step - loss: 0.0279 - accuracy: 0.9892\n",
      "Epoch 31/50\n",
      "10/10 [==============================] - 3s 294ms/step - loss: 0.0275 - accuracy: 0.9892\n",
      "Epoch 32/50\n",
      "10/10 [==============================] - 3s 294ms/step - loss: 0.0226 - accuracy: 0.9892\n",
      "Epoch 33/50\n",
      "10/10 [==============================] - 3s 296ms/step - loss: 0.0216 - accuracy: 0.9892\n",
      "Epoch 34/50\n",
      "10/10 [==============================] - 3s 308ms/step - loss: 0.0241 - accuracy: 0.9892\n",
      "Epoch 35/50\n",
      "10/10 [==============================] - 3s 294ms/step - loss: 0.0238 - accuracy: 0.9892\n",
      "Epoch 36/50\n",
      "10/10 [==============================] - 3s 296ms/step - loss: 0.0262 - accuracy: 0.9892\n",
      "Epoch 37/50\n",
      "10/10 [==============================] - 3s 297ms/step - loss: 0.0240 - accuracy: 0.9892\n",
      "Epoch 38/50\n",
      "10/10 [==============================] - 3s 296ms/step - loss: 0.0257 - accuracy: 0.9892\n",
      "Epoch 39/50\n",
      "10/10 [==============================] - 3s 303ms/step - loss: 0.0248 - accuracy: 0.9892\n",
      "Epoch 40/50\n",
      "10/10 [==============================] - 3s 308ms/step - loss: 0.0224 - accuracy: 0.9892\n",
      "Epoch 41/50\n",
      "10/10 [==============================] - 3s 304ms/step - loss: 0.0338 - accuracy: 0.9892\n",
      "Epoch 42/50\n",
      "10/10 [==============================] - 3s 301ms/step - loss: 0.0428 - accuracy: 0.9892\n",
      "Epoch 43/50\n",
      "10/10 [==============================] - 3s 302ms/step - loss: 0.0261 - accuracy: 0.9892\n",
      "Epoch 44/50\n",
      "10/10 [==============================] - 3s 296ms/step - loss: 0.0222 - accuracy: 0.9892\n",
      "Epoch 45/50\n",
      "10/10 [==============================] - 3s 296ms/step - loss: 0.0231 - accuracy: 0.9892\n",
      "Epoch 46/50\n",
      "10/10 [==============================] - 3s 312ms/step - loss: 0.0232 - accuracy: 0.9892\n",
      "Epoch 47/50\n",
      "10/10 [==============================] - 3s 310ms/step - loss: 0.0217 - accuracy: 0.9903\n",
      "Epoch 48/50\n",
      "10/10 [==============================] - 3s 303ms/step - loss: 0.0222 - accuracy: 0.9882\n",
      "Epoch 49/50\n",
      "10/10 [==============================] - 3s 306ms/step - loss: 0.0219 - accuracy: 0.9892\n",
      "Epoch 50/50\n",
      "10/10 [==============================] - 3s 307ms/step - loss: 0.0236 - accuracy: 0.9903\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "sup_df = pd.read_csv('suppliernames.txt')\n",
    "sup_df['SupplierName'] = sup_df['SupplierName'].apply(clean)\n",
    "\n",
    "\n",
    "query = read_invoice('invoice.txt')\n",
    "query = clean(query)\n",
    "\n",
    "\n",
    "model, tknzr  = train(suppliers =sup_df )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [<tf.Tensor: id=12352, shape=(117, 300), dtype=int32, numpy=\narray([[0, 0, 0, ..., 0, 0, 1],\n       [0, 0, 0, ..., 0, 0, 1],\n       [0, 0, 0, ..., 0, 0, 1],\n       ...,\n       [0, 0, 0, ..., 0, 0, 1],...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-67e89d67cedf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtknzr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msup_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-deac4a6f98c0>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(model, tknzr, suppliers, query)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_SEQ_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msupplier\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msupplier\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuppliers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-deac4a6f98c0>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_SEQ_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msupplier\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msupplier\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuppliers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/pkg/lib/python3.8/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_select_training_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m     return func.predict(\n\u001b[0m\u001b[1;32m    901\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/pkg/lib/python3.8/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    458\u001b[0m   def predict(self, model, x, batch_size=None, verbose=0, steps=None,\n\u001b[1;32m    459\u001b[0m               callbacks=None, **kwargs):\n\u001b[0;32m--> 460\u001b[0;31m     return self._model_iteration(\n\u001b[0m\u001b[1;32m    461\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         steps=steps, callbacks=callbacks, **kwargs)\n",
      "\u001b[0;32m/usr/pkg/lib/python3.8/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;31m# Enter tf.distribute.Strategy scope.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m       adapter = _process_inputs(\n\u001b[0m\u001b[1;32m    390\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m           \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/pkg/lib/python3.8/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    585\u001b[0m   \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0madapter_cls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_ADAPTER_FOR_STANDARDIZE_USER_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m     x, y, sample_weights = model._standardize_user_data(\n\u001b[0m\u001b[1;32m    588\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/pkg/lib/python3.8/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2465\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetV1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2466\u001b[0m       \u001b[0;31m# TODO(fchollet): run static checks with dataset output shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2467\u001b[0;31m       x = training_utils.standardize_input_data(\n\u001b[0m\u001b[1;32m   2468\u001b[0m           \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2469\u001b[0m           \u001b[0mfeed_input_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/pkg/lib/python3.8/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    524\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m       raise ValueError('Error when checking model ' + exception_prefix +\n\u001b[0m\u001b[1;32m    527\u001b[0m                        \u001b[0;34m': the list of Numpy arrays that you are passing to '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m                        \u001b[0;34m'your model is not the size the model expected. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [<tf.Tensor: id=12352, shape=(117, 300), dtype=int32, numpy=\narray([[0, 0, 0, ..., 0, 0, 1],\n       [0, 0, 0, ..., 0, 0, 1],\n       [0, 0, 0, ..., 0, 0, 1],\n       ...,\n       [0, 0, 0, ..., 0, 0, 1],..."
     ]
    }
   ],
   "source": [
    "\n",
    "predict(model, tknzr,sup_df, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
